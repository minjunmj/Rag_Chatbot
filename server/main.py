import os
import uuid
import json
import logging
import traceback
import tempfile
from shutil import which

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from fastapi import FastAPI, UploadFile, File, HTTPException, Request
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from dotenv import load_dotenv
from typing import Literal, Optional

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory

from faster_whisper import WhisperModel
import subprocess

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹… & ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log = logging.getLogger("uvicorn.error")
log.setLevel(logging.INFO)

app = FastAPI()

# ëª¨ë“  ì˜ˆì™¸ ë¡œê·¸ (HTTPException í¬í•¨í•´ì„œ ë³„ë„ë¡œë„ ì°ìŒ)
@app.exception_handler(Exception)
async def all_exception_handler(request: Request, exc: Exception):
    log.error("ğŸ’¥ Unhandled exception at %s %s", request.method, request.url.path)
    log.error("Headers: %s", dict(request.headers))
    log.error("Detail: %s", str(exc))
    log.error("Traceback:\n%s", "".join(traceback.format_exc()))
    return JSONResponse(
        status_code=500,
        content={"detail": "internal error", "exc_type": type(exc).__name__, "exc_msg": str(exc)},
    )

from fastapi.exceptions import HTTPException as FastAPIHTTPException

@app.exception_handler(FastAPIHTTPException)
async def http_exception_logger(request: Request, exc: FastAPIHTTPException):
    # HTTPExceptionë„ ì½˜ì†”ì— ì´ìœ ë¥¼ ë‚¨ê¹€
    log.error("âš ï¸ HTTPException %s %s -> %s", request.method, request.url.path, exc.detail)
    return JSONResponse(status_code=exc.status_code, content={"detail": exc.detail})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    log.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7, openai_api_key=OPENAI_API_KEY)
llm_router = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0, openai_api_key=OPENAI_API_KEY)

TOP_K = 3
# MODEL_PATH = "./model_bs32"
# INDEX_PATH = r"C:\Users\vkslr\Rag-Agent\voice_chatbot\server\store"
# RAW_DATA_ROOT = r"C:\Users\vkslr\Rag-Agent\voice_chatbot\server\data\Training\01.ì›ì²œë°ì´í„°"
MODEL_PATH    = os.getenv("MODEL_PATH", "/app/model_bs32")
INDEX_PATH    = os.getenv("INDEX_PATH", "/app/store")
RAW_DATA_ROOT = os.getenv("RAW_DATA_ROOT", "/app/data")

def assert_ffmpeg():
    if which("ffmpeg") is None:
        raise RuntimeError("ffmpeg not found. Windows: 'winget install Gyan.FFmpeg' í›„ PATH ë“±ë¡ í•„ìš”")

assert_ffmpeg()

# ì„ë² ë”©/FAISS
embedding_model = HuggingFaceEmbeddings(model_name=MODEL_PATH)
vectorstore = FAISS.load_local(INDEX_PATH, embeddings=embedding_model, allow_dangerous_deserialization=True)
log.info("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: %s", INDEX_PATH)

# Whisper: CPU ê°•ì œ í† ê¸€(ë¬¸ì œ ì§„ë‹¨ìš©)
USE_CPU_WHISPER = os.environ.get("USE_CPU_WHISPER", "0") == "1"

def init_whisper():
    device = "cpu"
    compute_type = "int8"
    if not USE_CPU_WHISPER:
        try:
            import torch
            if torch.cuda.is_available() and torch.backends.cudnn.is_available():
                device = "cuda"
                compute_type = "float16"
        except Exception as e:
            log.warning("CUDA ì²´í¬ ì‹¤íŒ¨. CPUë¡œ ì§„í–‰: %s", e)
    log.info("ğŸ§ Whisper ì´ˆê¸°í™”: device=%s, compute_type=%s", device, compute_type)
    return WhisperModel("large-v2", device=device, compute_type=compute_type)

stt_model = init_whisper()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ffmpegë¡œ WAV ë³€í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ffmpeg_to_wav(in_path: str, out_path: str, sr: int = 16000):
    # -vn: ì˜ìƒ ì œê±°, -ac 1: ëª¨ë…¸, -ar: ìƒ˜í”Œë ˆì´íŠ¸, -f wav: í¬ë§·
    cmd = ["ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
           "-i", in_path, "-vn", "-ac", "1", "-ar", str(sr), "-f", "wav", out_path]
    log.info("ğŸ› ï¸ ffmpeg ë³€í™˜ ì‹œì‘: %s", " ".join(cmd))
    cp = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if cp.returncode != 0 or not os.path.exists(out_path):
        err = cp.stderr.decode(errors="ignore")
        raise RuntimeError(f"ffmpeg ë³€í™˜ ì‹¤íŒ¨(code={cp.returncode}): {err[:800]}")
    log.info("âœ… ffmpeg ë³€í™˜ ì„±ê³µ â†’ %s", out_path)

# Chat_History
_STORE = {}
def get_session_history(session_id: str):
    if session_id not in _STORE:
        _STORE[session_id] = InMemoryChatMessageHistory()
    return _STORE[session_id]

def get_session_id(request: Request) -> str:
    return (
        request.headers.get("X-Session-Id")
        or request.query_params.get("session_id")
        or "anon"
    )

# Direct or Rag
class RouteDecision(BaseModel):
    route : Literal["RAG", "DIRECT"]
    reason : Optional[str] = None

router_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        (
            "ë‹¹ì‹ ì€ 'ë²•ë¥  íŒë¡€ Q&A' ì‹œìŠ¤í…œì˜ ë¼ìš°í„°ì…ë‹ˆë‹¤. ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. "
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‚´ë¶€ ë²•ë¥  ì§€ì‹ë² ì´ìŠ¤(ë²¡í„°ìŠ¤í† ì–´: êµ­ë‚´ íŒë¡€ ì›ë¬¸ JSON(ì‚¬ê±´ë²ˆí˜¸/ë©”íƒ€ í¬í•¨), "
            "íŒì‹œì‚¬í•­Â·íŒê²°ìš”ì§€Â·íŒë¡€ë‚´ìš© ë°œì·Œ, ë‚´ë¶€ í•´ì„¤/ìš”ì•½ ë¬¸ì„œ) ì¡°íšŒê°€ í•„ìš”í•œì§€ ê²°ì •í•˜ì„¸ìš”.\n"
            "ë‹¤ìŒì˜ ê²½ìš°ì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ RAGë¥¼ ì„ íƒí•˜ì„¸ìš” (ëª¨í˜¸í•˜ë©´ RAG):\n"
            " 1) íŠ¹ì • ë²•ë ¹/ì¡°ë¬¸/ìš”ê±´/êµ¬ì„±ìš”ê±´ ì¶©ì¡± ì—¬ë¶€ íŒë‹¨(ì˜ˆ: '~ë²•ìƒ ~í•œ ê²½ìš° ì²˜ë²Œ ê°€ëŠ¥?', '~ì£„ ì„±ë¦½?')\n"
            " 2) ì‚¬ì‹¤ê´€ê³„ë¥¼ ì œì‹œí•˜ê³  ê·¸ ì‚¬ì‹¤ì— ë¶€í•©í•˜ëŠ” íŒë¡€/ì‚¬ë¡€ë¥¼ ìš”êµ¬\n"
            " 3) 'ê´€ë ¨/ìœ ì‚¬ íŒë¡€', 'ê·¼ê±°/ì¶œì²˜', 'ìµœì‹ /ìµœê·¼ íŒë¡€', 'ì¸ìš©'ì„ ëª…ì‹œì ìœ¼ë¡œ ìš”êµ¬\n"
            " 4) ì‚¬ê±´ë²ˆí˜¸Â·ì‚¬ê±´ëª…Â·ì„ ê³ ì¼Â·ë²•ì› ë“± íŒë¡€ ì‹ë³„ì •ë³´ë¥¼ ìš”êµ¬/ì–¸ê¸‰\n"
            " 5) íŒê²°ë¬¸ ë‚´ìš©Â·íŒì‹œì‚¬í•­Â·íŒê²°ìš”ì§€ ì›ë¬¸ í™•ì¸ì´ í•„ìš”í•œ ì§ˆë¬¸\n"
            " 6) í–‰ì •/í˜•ì‚¬/ë¯¼ì‚¬ì—ì„œ ì œì¬Â·ì²˜ë²ŒÂ·ê³¼íƒœë£ŒÂ·ë²Œê¸ˆÂ·ë¬´íš¨/ì·¨ì†Œ ê°€ëŠ¥ì„± íŒë‹¨ ì§ˆì˜\n"
            " 7) íŠ¹ì • ë²•ë ¹ëª…(ì˜ˆ: ì‹í’ˆìœ„ìƒë²•, ì•½ì‚¬ë²•, ì—¼ê´€ë¦¬ë²• ë“±)ì´ë‚˜ ì¡°ë¬¸ì„ ì§ì ‘ ì–¸ê¸‰\n"
            "ìœ„ì— í•´ë‹¹í•˜ì§€ ì•Šê³  ì¼ë°˜ ê°œë… ì„¤ëª…(ìš©ì–´ ì •ì˜, ì ˆì°¨ ê°œìš” ë“±)ë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ë©´ DIRECTë¥¼ ì„ íƒí•˜ì„¸ìš”.\n"
            "- í•„ìš”í•˜ë©´ ì •í™•íˆ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥: {{\"route\":\"RAG\",\"reason\":\"...\"}}\n"
            "- í•„ìš” ì—†ìœ¼ë©´ ì •í™•íˆ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥: {{\"route\":\"DIRECT\",\"reason\":\"...\"}}\n"
            "ì¶”ê°€ ì„¤ëª… ê¸ˆì§€. ëª¨í˜¸í•˜ê±°ë‚˜ í™•ì‹ ì´ ì—†ìœ¼ë©´ RAGë¥¼ ì„ íƒí•˜ì„¸ìš”."
        ),
    ),
    # (ì„ íƒ) ë¼ìš°í„°ë„ ì§ì „ ë¬¸ë§¥ì„ ì°¸ê³ í•˜ë ¤ë©´ historyë¥¼ í¬í•¨í•˜ì„¸ìš”:
    # MessagesPlaceholder("history"),
    ("human", "ì§ˆë¬¸:\n{question}"),
])
router_chain = router_prompt | llm_router.with_structured_output(RouteDecision)

# âœ… ì§ì ‘ ë‹µë³€ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´)
direct_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        (
            "ë‹¹ì‹ ì€ ë„ì›€ì„ ì£¼ëŠ” ì¡°ìˆ˜ì…ë‹ˆë‹¤. ì™¸ë¶€/ì‚¬ì„¤ ìë£Œ(ì‚¬ìš©ì íŒŒì¼Â·ì½”ë“œÂ·ë¬¸ì„œ ë“±)ë¥¼ ì¡°íšŒí•˜ì§€ ë§ê³ , "
            "ì¼ë°˜ ì§€ì‹ê³¼ ì£¼ì–´ì§„ ë§¥ë½ë§Œìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
            "ë§Œì•½ ì‚¬ì„¤ ìë£Œ ì—†ì´ëŠ” í™•ì •í•˜ê¸° ì–´ë ¤ìš´ ë¶€ë¶„ì´ ìˆë‹¤ë©´, ê·¸ ì‚¬ì‹¤ì„ í•œ ì¤„ë¡œë§Œ ì§§ê²Œ ë°í˜€ì£¼ì„¸ìš”."
        ),
    ),
    MessagesPlaceholder("history"),
    ("human", "{question}"),
])

direct_chain = direct_prompt | llm

direct_chain_with_history = RunnableWithMessageHistory(
    direct_chain,
    get_session_history,          # ì„¸ì…˜ID â†’ íˆìŠ¤í† ë¦¬ ê°ì²´
    input_messages_key="question",
    history_messages_key="history",
)

# ë¹„ë™ê¸° í—¬í¼ (ê·¸ëŒ€ë¡œ ì‚¬ìš©)
async def llm_route_decision(question: str) -> RouteDecision:
    return await router_chain.ainvoke({"question": question})

async def llm_direct_answer(question: str, *, session_id: str) -> str:
    hist = get_session_history(session_id)
    log.info("ğŸ•˜ BEFORE DIRECT history_len=%d", len(hist.messages))
    resp = await direct_chain_with_history.ainvoke({"question": question}, config={"configurable": {"session_id": session_id}})
    log.info("ğŸ•™ AFTER  DIRECT history_len=%d", len(hist.messages))
    return getattr(resp, "content", str(resp))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class Query(BaseModel):
    question: str

def chat_rag(query: Query):
    docs = vectorstore.similarity_search(query.question, k=TOP_K)
    docs_name = [f"{d.metadata.get('ì‚¬ê±´ë²ˆí˜¸')}.json" for d in docs if d.metadata.get("ì‚¬ê±´ë²ˆí˜¸")]

    contexts = []
    for subdir in os.listdir(RAW_DATA_ROOT):
        subdir_path = os.path.join(RAW_DATA_ROOT, subdir)
        if not os.path.isdir(subdir_path):
            continue
        for filename in os.listdir(subdir_path):
            if filename in docs_name:
                file_path = os.path.join(subdir_path, filename)
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    íŒì‹œì‚¬í•­ = (data.get("íŒì‹œì‚¬í•­") or "").strip()
                    íŒê²°ìš”ì§€ = (data.get("íŒê²°ìš”ì§€") or "").strip()
                    íŒë¡€ë‚´ìš© = (data.get("íŒë¡€ë‚´ìš©") or "").strip()
                    if any([íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€, íŒë¡€ë‚´ìš©]):
                        contexts.append(
                            f"[íŒŒì¼ëª…: {filename}]\n[íŒì‹œì‚¬í•­]\n{íŒì‹œì‚¬í•­}\n[íŒê²°ìš”ì§€]\n{íŒê²°ìš”ì§€}\n[íŒë¡€ë‚´ìš©]\n{íŒë¡€ë‚´ìš©}\n"
                        )
                except Exception as e:
                    log.warning("âŒ ì›ë¬¸ ë¡œë“œ ì‹¤íŒ¨: %s â†’ %s", file_path, e)

    final_context = "\n\n---\n\n".join(contexts) if contexts else "(ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ)"
    prompt = f"""ë‹¤ìŒì€ ì§ˆë¬¸ì— ê´€í•œ ìµœëŒ€ {TOP_K}ê°œ íŒë¡€ ë°œì·Œì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ í™œìš©í•´ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
ë‹µë³€ ë§ˆì§€ë§‰ ì¤„ì— ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
"ê´€ë ¨ëœ íŒë¡€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: [íŒŒì¼ëª…1, íŒŒì¼ëª…2, ...]"

{final_context}

[ì§ˆë¬¸]
{query.question}
"""
    resp = llm.invoke(prompt)
    return {"answer": resp.content, "used_files": docs_name}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¼ìš°íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.makedirs("temp", exist_ok=True)

@app.post("/voice-chat")
async def voice_chat(request: Request, file: UploadFile = File(...)):
    session_id = get_session_id(request)
    log.info("ğŸ‘¤ session_id=%s", session_id)

    log.info("ğŸ“¥ /voice-chat ì—…ë¡œë“œ: filename=%s, content_type=%s", file.filename, file.content_type)

    uid = uuid.uuid4().hex
    orig_ext = os.path.splitext(file.filename or "")[1].lower() or ".bin"
    in_path = os.path.join("temp", f"{uid}{orig_ext}")
    wav_path = os.path.join("temp", f"{uid}.wav")

    try:
        # 1) ì €ì¥
        raw = await file.read()
        with open(in_path, "wb") as f:
            f.write(raw)
        log.info("ğŸ“„ ì €ì¥ ì™„ë£Œ: %s (%d bytes)", in_path, len(raw))

        # 2) ì˜¤ë””ì˜¤ â†’ WAV(ffmpeg)
        ffmpeg_to_wav(in_path, wav_path, sr=16000)

        # 3) STT
        log.info("ğŸ—£ï¸ STT ì‹œì‘")
        segments, info = stt_model.transcribe(wav_path)
        texts = []
        for seg in segments:
            t = getattr(seg, "text", None)
            if t is None and isinstance(seg, dict):
                t = seg.get("text")
            if t:
                texts.append(t.strip())
        question_text = " ".join(texts) if texts else "(ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
        log.info("ğŸ—£ï¸ STT ê²°ê³¼ ê¸¸ì´=%d", len(question_text))

        # Decision
        decision = await llm_route_decision(question_text)
        log.info("ğŸ§­ ë¼ìš°íŒ…: %s (%s)", decision.route, decision.reason)

        # 4) RAG
        if decision.route == "RAG":
            log.info("ğŸ” RAG ê²€ìƒ‰ ì‹œì‘")
            rag_resp = chat_rag(Query(question=question_text))
            used = rag_resp.get("used_files", [])
            answer = rag_resp.get("answer", "")
            log.info("âœ… RAG ì™„ë£Œ, ì‚¬ìš© íŒŒì¼: %s", used)

            hist = get_session_history(session_id)
            log.info("ğŸ•˜ BEFORE RAG   history_len=%d", len(hist.messages))
            hist.add_user_message(question_text)
            hist.add_ai_message(answer)
            log.info("ğŸ•™ AFTER  RAG   history_len=%d", len(hist.messages))

            return {"question": question_text, "answer": rag_resp["answer"], "used_files": used}

        if decision.route == "DIRECT":
            log.info("ğŸ’¬ DIRECT ë‹µë³€ ìƒì„±")
            answer = await llm_direct_answer(question_text, session_id=session_id)
            return {"question": question_text, "answer": answer, "used_files": []}
    
    except Exception as e:
        # ì—¬ê¸°ì„œ ì¡íˆë©´ ìœ„ì˜ all_exception_handlerê°€ ìŠ¤íƒê¹Œì§€ ì°ì–´ì¤Œ
        raise HTTPException(status_code=500, detail=f"voice-chat failed: {e}")

    finally:
        for p in (in_path, wav_path):
            try:
                if p and os.path.exists(p):
                    os.remove(p)
            except:
                pass
