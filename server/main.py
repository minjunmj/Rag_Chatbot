import os
import uuid
import json
import logging
import traceback
import asyncio
# import tempfile
from functools import lru_cache
from shutil import which
# import threading
import secrets  

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Depends
from fastapi.responses import JSONResponse, RedirectResponse, Response
from pydantic import BaseModel
from dotenv import load_dotenv
from typing import Literal, Optional, Dict, Any  # [FIX] íƒ€ì… ì‚¬ìš©

from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from sentence_transformers import CrossEncoder
import torch
import numpy as np

from faster_whisper import WhisperModel
import subprocess

HEARTBEAT_SEC = int(os.getenv("HEARTBEAT_SEC", "25"))
stt_model: WhisperModel | None = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹… & ì•±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log = logging.getLogger("uvicorn.error")
log.setLevel(logging.INFO)

app = FastAPI()

# ëª¨ë“  ì˜ˆì™¸ ë¡œê·¸ (HTTPException í¬í•¨í•´ì„œ ë³„ë„ë¡œë„ ì°ìŒ)
@app.exception_handler(Exception)
async def all_exception_handler(request: Request, exc: Exception):
    log.error("ğŸ’¥ Unhandled exception at %s %s", request.method, request.url.path)
    log.error("Headers: %s", dict(request.headers))
    log.error("Detail: %s", str(exc))
    log.error("Traceback:\n%s", "".join(traceback.format_exc()))
    return JSONResponse(
        status_code=500,
        content={"detail": "internal error", "exc_type": type(exc).__name__, "exc_msg": str(exc)},
        headers={"Content-Type": "application/json; charset=utf-8"},
    )

from fastapi.exceptions import HTTPException as FastAPIHTTPException

@app.exception_handler(FastAPIHTTPException)
async def http_exception_logger(request: Request, exc: FastAPIHTTPException):
    # HTTPExceptionë„ ì½˜ì†”ì— ì´ìœ ë¥¼ ë‚¨ê¹€
    log.error("âš ï¸ HTTPException %s %s -> %s", request.method, request.url.path, exc.detail)
    return JSONResponse(status_code=exc.status_code, content={"detail": exc.detail},
                        headers={"Content-Type": "application/json; charset=utf-8"})

@app.get("/healthz")
def healthz():
    return {"status": "ok"}

@app.get("/", include_in_schema=False)
def index():
    return RedirectResponse(url="/docs")

@app.get("/favicon.ico", include_in_schema=False)
def favicon():
    return Response(status_code=204)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¤€ë¹„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    log.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# gpt apiì‚¬ìš©
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.7, openai_api_key=OPENAI_API_KEY)
llm_router = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.0, openai_api_key=OPENAI_API_KEY)

TOP_K = 3
MODEL_PATH    = os.getenv("MODEL_PATH", "/app/model_bs32")
INDEX_PATH    = os.getenv("INDEX_PATH", "/app/store")
RAW_DATA_ROOT = os.getenv("RAW_DATA_ROOT", "/app/data")

# ğŸ”¸ (ì‹ ê·œ) RAG ìœ ì‚¬ë„ ì„ê³„ì¹˜ í™˜ê²½ë³€ìˆ˜
RERANK_THRESHOLD = float(os.getenv("RERANK_THRESHOLD", "0.20"))

def assert_ffmpeg():
    if which("ffmpeg") is None:
        raise RuntimeError("ffmpeg not found. Windows: 'winget install Gyan.FFmpeg' í›„ PATH ë“±ë¡ í•„ìš”")

assert_ffmpeg()

# ì„ë² ë”©/FAISS
embedding_model = HuggingFaceEmbeddings(model_name=MODEL_PATH)
vectorstore = FAISS.load_local(INDEX_PATH, embeddings=embedding_model, allow_dangerous_deserialization=True)
log.info("âœ… FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: %s", INDEX_PATH)

# Whisper: CPU ê°•ì œ í† ê¸€(ë¬¸ì œ ì§„ë‹¨ìš©)
USE_CPU_WHISPER = os.environ.get("USE_CPU_WHISPER", "0") == "1"

def init_whisper():
    device = "cpu"
    compute_type = "int8"
    if not USE_CPU_WHISPER:
        try:
            import torch
            if torch.cuda.is_available() and torch.backends.cudnn.is_available():
                device = "cuda"
                compute_type = "float16"
        except Exception as e:
            log.warning("CUDA ì²´í¬ ì‹¤íŒ¨. CPUë¡œ ì§„í–‰: %s", e)
    log.info("ğŸ§ Whisper ì´ˆê¸°í™”: device=%s, compute_type=%s", device, compute_type)
    return WhisperModel("base", device=device, compute_type=compute_type)

@lru_cache(maxsize=1)
def get_whisper():
    global stt_model
    if stt_model is None:
        stt_model = init_whisper()
    return stt_model

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ffmpegë¡œ WAV ë³€í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ffmpeg_to_wav(in_path: str, out_path: str, sr: int = 16000):
    # -vn: ì˜ìƒ ì œê±°, -ac 1: ëª¨ë…¸, -ar: ìƒ˜í”Œë ˆì´íŠ¸, -f wav: í¬ë§·
    cmd = ["ffmpeg", "-y", "-hide_banner", "-loglevel", "error",
           "-i", in_path, "-vn", "-ac", "1", "-ar", str(sr), "-f", "wav", out_path]
    log.info("ğŸ› ï¸ ffmpeg ë³€í™˜ ì‹œì‘: %s", " ".join(cmd))
    cp = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    if cp.returncode != 0 or not os.path.exists(out_path):
        err = cp.stderr.decode(errors="ignore")
        raise RuntimeError(f"ffmpeg ë³€í™˜ ì‹¤íŒ¨(code={cp.returncode}): {err[:800]}")
    log.info("âœ… ffmpeg ë³€í™˜ ì„±ê³µ â†’ %s", out_path)

# Chat_History ê·¸ì „ ëŒ€í™”ë¥¼ ê¸°ë¡ 
_STORE = {}
def get_session_history(session_id: str):
    if session_id not in _STORE:
        _STORE[session_id] = InMemoryChatMessageHistory()
    return _STORE[session_id]

def get_session_id(request: Request) -> str:
    return (
        request.headers.get("X-Session-Id")
        or request.query_params.get("session_id")
        or "anon"
    )

# Direct or Rag ë¼ìš°í„°
class RouteDecision(BaseModel):
    route : Literal["RAG", "DIRECT"]
    reason : Optional[str] = None

router_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        (
            "ë‹¹ì‹ ì€ 'ë²•ë¥  íŒë¡€ Q&A' ì‹œìŠ¤í…œì˜ ë¼ìš°í„°ì…ë‹ˆë‹¤. ì¶œë ¥ì€ ë°˜ë“œì‹œ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. "
            "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‚´ë¶€ ë²•ë¥  ì§€ì‹ë² ì´ìŠ¤(ë²¡í„°ìŠ¤í† ì–´: êµ­ë‚´ íŒë¡€ ì›ë¬¸ JSON(ì‚¬ê±´ë²ˆí˜¸/ë©”íƒ€ í¬í•¨), "
            "íŒì‹œì‚¬í•­Â·íŒê²°ìš”ì§€Â·íŒë¡€ë‚´ìš© ë°œì·Œ, ë‚´ë¶€ í•´ì„¤/ìš”ì•½ ë¬¸ì„œ) ì¡°íšŒê°€ í•„ìš”í•œì§€ ê²°ì •í•˜ì„¸ìš”.\n"
            "ë‹¤ìŒì˜ ê²½ìš°ì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ RAGë¥¼ ì„ íƒí•˜ì„¸ìš” (ëª¨í˜¸í•˜ë©´ RAG):\n"
            " 1) íŠ¹ì • ë²•ë ¹/ì¡°ë¬¸/ìš”ê±´/êµ¬ì„±ìš”ê±´ ì¶©ì¡± ì—¬ë¶€ íŒë‹¨\n"
            " 2) ì‚¬ì‹¤ê´€ê³„ë¥¼ ì œì‹œí•˜ê³  ê·¸ ì‚¬ì‹¤ì— ë¶€í•©í•˜ëŠ” íŒë¡€/ì‚¬ë¡€ ìš”êµ¬\n"
            " 3) 'ê´€ë ¨/ìœ ì‚¬ íŒë¡€', 'ê·¼ê±°/ì¶œì²˜', 'ìµœì‹ /ìµœê·¼ íŒë¡€', 'ì¸ìš©' ìš”êµ¬\n"
            " 4) íŒê²°ë¬¸ ë‚´ìš©Â·íŒì‹œì‚¬í•­Â·íŒê²°ìš”ì§€ ì›ë¬¸ í™•ì¸ í•„ìš”\n"
            " 5) í–‰ì •/í˜•ì‚¬/ë¯¼ì‚¬ ì œì¬Â·ì²˜ë²ŒÂ·ê³¼íƒœë£ŒÂ·ë²Œê¸ˆÂ·ë¬´íš¨/ì·¨ì†Œ íŒë‹¨ ì§ˆì˜\n"
            " 6) íŠ¹ì • ë²•ë ¹ëª…ì´ë‚˜ ì¡°ë¬¸ì„ ì§ì ‘ ì–¸ê¸‰\n"
            "- í•„ìš”í•˜ë©´ ì •í™•íˆ: {{\"route\":\"RAG\",\"reason\":\"...\"}}\n"
            "- í•„ìš” ì—†ìœ¼ë©´ ì •í™•íˆ: {{\"route\":\"DIRECT\",\"reason\":\"...\"}}\n"
            "ì¶”ê°€ ì„¤ëª… ê¸ˆì§€. ëª¨í˜¸í•˜ë©´ RAG."
        ),
    ),
    ("human", "ì§ˆë¬¸:\n{question}"),
])
router_chain = router_prompt | llm_router.with_structured_output(RouteDecision)

# DIRECT í”„ë¡¬í”„íŠ¸
direct_prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        (
            "ë‹¹ì‹ ì€ ë„ì›€ì„ ì£¼ëŠ” ì¡°ìˆ˜ì…ë‹ˆë‹¤. ì™¸ë¶€/ì‚¬ì„¤ ìë£Œ(ì‚¬ìš©ì íŒŒì¼Â·ì½”ë“œÂ·ë¬¸ì„œ ë“±)ë¥¼ ì¡°íšŒí•˜ì§€ ë§ê³ , "
            "ì¼ë°˜ ì§€ì‹ê³¼ ì£¼ì–´ì§„ ë§¥ë½ë§Œìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
            "ë§Œì•½ ì‚¬ì„¤ ìë£Œ ì—†ì´ëŠ” í™•ì •í•˜ê¸° ì–´ë ¤ìš´ ë¶€ë¶„ì´ ìˆë‹¤ë©´, ê·¸ ì‚¬ì‹¤ì„ í•œ ì¤„ë¡œë§Œ ì§§ê²Œ ë°í˜€ì£¼ì„¸ìš”."
            "ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”."
        ),
    ),
    MessagesPlaceholder("history"),
    ("human", "{question}"),
])

direct_chain = direct_prompt | llm

direct_chain_with_history = RunnableWithMessageHistory(
    direct_chain,
    get_session_history,
    input_messages_key="question",
    history_messages_key="history",
)

# ë¹„ë™ê¸° í—¬í¼
async def llm_route_decision(question: str) -> RouteDecision:
    return await router_chain.ainvoke({"question": question})

async def llm_direct_answer(question: str, *, session_id: str) -> str:
    hist = get_session_history(session_id)
    log.info("ğŸ•˜ BEFORE DIRECT history_len=%d", len(hist.messages))
    resp = await direct_chain_with_history.ainvoke({"question": question}, config={"configurable": {"session_id": session_id}})
    log.info("ğŸ•™ AFTER  DIRECT history_len=%d", len(hist.messages))
    return getattr(resp, "content", str(resp))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAG
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    _DEVICE = "cuda" if (not USE_CPU_WHISPER and torch.cuda.is_available() and torch.backends.cudnn.is_available()) else "cpu"
except Exception:
    _DEVICE = "cpu"
cross_encoder = CrossEncoder("BAAI/bge-reranker-v2-m3", device=_DEVICE, max_length=512)

class Query(BaseModel):
    question: str

def chat_rag(query: Query):
    q_text = query.question
    candidates = vectorstore.similarity_search(q_text, k=50)

    # ì¤‘ë³µ ì œê±° (ì‚¬ê±´ë²ˆí˜¸ ê¸°ì¤€)
    seen = set()
    uniq_candidates = []
    for d in candidates:
        case_no = d.metadata.get("ì‚¬ê±´ë²ˆí˜¸")
        if case_no in seen:
            continue
        seen.add(case_no)
        uniq_candidates.append(d)
    candidates = uniq_candidates[:50]

    q_emb = np.asarray(embedding_model.embed_query(q_text), dtype=np.float32)
    doc_texts = [d.page_content for d in candidates]
    doc_embs  = np.asarray(embedding_model.embed_documents(doc_texts), dtype=np.float32)

    def _l2norm(x):
        n = np.linalg.norm(x, axis=1, keepdims=True).astype(np.float32)
        np.maximum(n, 1e-12, out=n)
        return x / n

    q = _l2norm(q_emb.reshape(1, -1))[0]
    D = _l2norm(doc_embs)

    lambda_mult = 0.9
    top_mmr = 10
    selected = []
    unselected = set(range(len(D)))
    sims = D @ q

    while len(selected) < min(top_mmr, len(D)):
        if not selected:
            i = int(np.argmax(sims))
            selected.append(i); unselected.remove(i)
            continue
        max_div = np.max(D[list(selected)] @ D[list(unselected)].T, axis=0)
        mmr_scores = lambda_mult * sims[list(unselected)] - (1 - lambda_mult) * max_div
        pick_idx_in_un = int(np.argmax(mmr_scores))
        i = list(unselected)[pick_idx_in_un]
        selected.append(i); unselected.remove(i)

    stage2_docs = [candidates[i] for i in selected]

    # rerank
    pairs  = [(q_text, d.page_content) for d in stage2_docs]
    scores = cross_encoder.predict(pairs, batch_size=64, show_progress_bar=False)
    order  = np.argsort(scores)[::-1][:TOP_K]

    used_docs = [stage2_docs[i] for i in order]
    top_scores = [float(scores[i]) for i in order] if len(order) > 0 else []
    best_score = max(top_scores) if top_scores else -1e9

    # ì„ê³„ì¹˜: ë‚®ìœ¼ë©´ 'ê´€ë ¨ ì—†ìŒ'
    if not np.isfinite(best_score) or best_score < RERANK_THRESHOLD or len(used_docs) == 0:
        # ì§ˆë¬¸ë§Œìœ¼ë¡œ ê°„ë‹¨ ë‹µë³€
        prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë‚´ë¶€ íŒë¡€ì—ì„œ ëšœë ·í•œ ê´€ë ¨ ê·¼ê±°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
ê°€ëŠ¥í•œ ë²”ìœ„ì—ì„œ ì¼ë°˜ì  ì„¤ëª…ì„ í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ì œì‹œí•˜ì„¸ìš”.

[ì§ˆë¬¸]
{q_text}
"""
        resp = llm.invoke(prompt)
        answer = getattr(resp, "content", str(resp))
        # ì„œë²„ê°€ ëª…í™•íˆ í‘œê¸°
        answer += "\n\nê´€ë ¨ëœ íŒë¡€ëŠ” ì—†ìŠµë‹ˆë‹¤."
        return {"answer": answer, "used_files": []}

    # íŒŒì¼ëª… ìˆ˜ì§‘
    docs_name = []
    for d in used_docs:
        cn = d.metadata.get("ì‚¬ê±´ë²ˆí˜¸")
        if cn:
            docs_name.append(f"{cn}.json")

    # ì›ë¬¸ ë¡œë”©
    contexts = []
    if docs_name:
        for subdir in os.listdir(RAW_DATA_ROOT):
            subdir_path = os.path.join(RAW_DATA_ROOT, subdir)
            if not os.path.isdir(subdir_path):
                continue
            for filename in os.listdir(subdir_path):
                if filename in docs_name:
                    file_path = os.path.join(subdir_path, filename)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            data = json.load(f)
                        íŒì‹œì‚¬í•­ = (data.get("íŒì‹œì‚¬í•­") or "").strip()
                        íŒê²°ìš”ì§€ = (data.get("íŒê²°ìš”ì§€") or "").strip()
                        íŒë¡€ë‚´ìš© = (data.get("íŒë¡€ë‚´ìš©") or "").strip()
                        if any([íŒì‹œì‚¬í•­, íŒê²°ìš”ì§€, íŒë¡€ë‚´ìš©]):
                            contexts.append(
                                f"[íŒŒì¼ëª…: {filename}]\n[íŒì‹œì‚¬í•­]\n{íŒì‹œì‚¬í•­}\n[íŒê²°ìš”ì§€]\n{íŒê²°ìš”ì§€}\n[íŒë¡€ë‚´ìš©]\n{íŒë¡€ë‚´ìš©}\n"
                            )
                    except Exception as e:
                        log.warning("âŒ ì›ë¬¸ ë¡œë“œ ì‹¤íŒ¨: %s â†’ %s", file_path, e)

    final_context = "\n\n---\n\n".join(contexts) if contexts else "(ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ)"
    # ë” ì´ìƒ LLMì—ê²Œ íŒŒì¼ëª… ì¶œë ¥ ê°•ì œí•˜ì§€ ì•ŠìŒ (ì„œë²„ê°€ ì§ì ‘ ë¶™ì„)
    prompt = f"""ë‹¤ìŒì€ ì§ˆë¬¸ì— ê´€í•œ íŒë¡€ ë°œì·Œì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ í™œìš©í•´ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ê°„ê²°íˆ ë‹µí•˜ì„¸ìš”.
ìë£Œê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ì¶”ì •/ì¼ë°˜ ì„¤ëª…ë„ í—ˆìš©ë©ë‹ˆë‹¤.

{final_context}

[ì§ˆë¬¸]
{q_text}
"""
    resp = llm.invoke(prompt)
    answer = getattr(resp, "content", str(resp))

    # ì„œë²„ì—ì„œ ì‹¤ì œ íŒŒì¼ëª…ì„ ì§ì ‘ ë§ë¶™ì„
    if docs_name:
        answer += "\n\nê´€ë ¨ëœ íŒë¡€ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: [" + ", ".join(docs_name) + "]"
    else:
        answer += "\n\nê´€ë ¨ëœ íŒë¡€ëŠ” ì—†ìŠµë‹ˆë‹¤."

    return {"answer": answer, "used_files": docs_name}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¼ìš°íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JOBS: Dict[str, Dict[str, Any]] = {}  # job_id -> {status, question, answer, used_files, message}

os.makedirs("temp", exist_ok=True)

@app.post("/voice-chat")
async def voice_chat(request: Request, file: UploadFile = File(...), stt_model: WhisperModel = Depends(get_whisper)):
    """
    ì—…ë¡œë“œ â†’ ì¦‰ì‹œ 202 + job_id ë°˜í™˜.
    ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì „ì²´ íŒŒì´í”„ë¼ì¸(ffmpeg â†’ STT â†’ ë¼ìš°íŒ… â†’ DIRECT/RAG)ì„ ìˆ˜í–‰.
    """
    session_id = get_session_id(request)
    log.info("ğŸ‘¤ session_id=%s", session_id)
    log.info("ğŸ“¥ /voice-chat ì—…ë¡œë“œ: filename=%s, content_type=%s", file.filename, file.content_type)

    uid = uuid.uuid4().hex
    orig_ext = os.path.splitext(file.filename or "")[1].lower() or ".bin"
    in_path = os.path.join("temp", f"{uid}{orig_ext}")
    wav_path = os.path.join("temp", f"{uid}.wav")

    # 1) íŒŒì¼ ì €ì¥
    raw = await file.read()
    with open(in_path, "wb") as f:
        f.write(raw)
    log.info("ğŸ“„ ì €ì¥ ì™„ë£Œ: %s (%d bytes)", in_path, len(raw))

    # 2) ì‘ì—… ë“±ë¡ + ë¹„ë™ê¸° ì‹¤í–‰
    job_id = secrets.token_hex(8)
    # ì²˜ìŒì—ëŠ” ì¤‘ë¦½ ë¬¸êµ¬
    # JOBS[job_id] = {"status": "processing", "message": "ì²˜ë¦¬ ì¤‘..."}
    JOBS[job_id] = {"status": "processing"}
    async def run_job():
        try:
            # ë³€í™˜
            ffmpeg_to_wav(in_path, wav_path, sr=16000)

            # STT
            log.info("ğŸ—£ï¸ STT ì‹œì‘")
            segments, info = stt_model.transcribe(wav_path)
            texts = []
            for seg in segments:
                t = getattr(seg, "text", None)
                if t is None and isinstance(seg, dict):
                    t = seg.get("text")
                if t:
                    texts.append(t.strip())
            question_text = " ".join(texts) if texts else "(ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
            log.info("ğŸ—£ï¸ STT ê²°ê³¼ ê¸¸ì´=%d", len(question_text))

            # ë¼ìš°íŒ…
            decision = await llm_route_decision(question_text)
            log.info("ğŸ§­ ë¼ìš°íŒ…: %s (%s)", decision.route, decision.reason)

            # ì§„í–‰ ìƒíƒœ ë¬¸êµ¬ ì—…ë°ì´íŠ¸: RAGë§Œ 'ìë£Œ ê²€ìƒ‰ì¤‘...'
            if decision.route == "RAG":
                JOBS[job_id]["message"] = "ìë£Œ ê²€ìƒ‰ì¤‘..."
            # else:
            #     JOBS[job_id]["message"] = "ë‹µë³€ ìƒì„±ì¤‘..."

            if decision.route == "DIRECT":
                log.info("ğŸ’¬ DIRECT ë‹µë³€ ìƒì„±")
                answer = await llm_direct_answer(question_text, session_id=session_id)
                JOBS[job_id] = {
                    "status": "done",
                    "question": question_text,
                    "answer": answer,
                    "used_files": []
                }
                return

            # RAG
            log.info("ğŸ” RAG ê²€ìƒ‰ ì‹œì‘")
            rag_resp = await asyncio.to_thread(chat_rag, Query(question=question_text))
            used = rag_resp.get("used_files", [])
            answer = rag_resp.get("answer", "")

            # íˆìŠ¤í† ë¦¬ ë°˜ì˜
            hist = get_session_history(session_id)
            log.info("ğŸ•˜ BEFORE RAG   history_len=%d", len(hist.messages))
            hist.add_user_message(question_text)
            hist.add_ai_message(answer)
            log.info("ğŸ•™ AFTER  RAG   history_len=%d", len(hist.messages))

            JOBS[job_id] = {
                "status": "done",
                "question": question_text,
                "answer": answer,
                "used_files": used
            }
        except Exception as e:
            log.error("âŒ ì‘ì—… ì‹¤íŒ¨: %s", e, exc_info=True)
            JOBS[job_id] = {"status": "error", "message": f"ì„œë²„ ì˜¤ë¥˜: {str(e)}"}
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for p in (in_path, wav_path):
                try:
                    if p and os.path.exists(p):
                        os.remove(p)
                except:
                    pass

    asyncio.create_task(run_job())

    # ì¦‰ì‹œ 202 ë°˜í™˜ (Cloudflare/ëª¨ë°”ì¼ íƒ€ì„ì•„ì›ƒ íšŒí”¼)
    return JSONResponse({"status": "processing", "job_id": job_id},
                        status_code=202,
                        headers={"Location": f"/voice-chat/result?job_id={job_id}",
                                 "Content-Type": "application/json; charset=utf-8"})

@app.get("/voice-chat/result")
async def voice_chat_result(job_id: str):
    job = JOBS.get(job_id)
    if not job:
        return JSONResponse({"status": "error", "message": "unknown job_id"}, status_code=404,
                            headers={"Content-Type": "application/json; charset=utf-8"})
    return JSONResponse(job, status_code=200, headers={"Content-Type": "application/json; charset=utf-8"})

# [FIX] ì•„ë˜ ë™ê¸° ì²˜ë¦¬ ë¼ìš°íŠ¸ëŠ” ë¹„ë™ê¸° ì¡ ë°©ì‹ê³¼ ê²½ë¡œ ì¶©ëŒí•˜ë¯€ë¡œ, ê²½ë¡œëª…ì„ ë³€ê²½
# @app.post("/voice-chat/sync")
# async def voice_chat_sync(request: Request, file: UploadFile = File(...), stt_model: WhisperModel = Depends(get_whisper)):
#     session_id = get_session_id(request)
#     log.info("ğŸ‘¤ session_id=%s", session_id)
#     log.info("ğŸ“¥ /voice-chat/sync ì—…ë¡œë“œ: filename=%s, content_type=%s", file.filename, file.content_type)

#     uid = uuid.uuid4().hex
#     orig_ext = os.path.splitext(file.filename or "")[1].lower() or ".bin"
#     in_path = os.path.join("temp", f"{uid}{orig_ext}")
#     wav_path = os.path.join("temp", f"{uid}.wav")
#     try:
#         # 1) ì €ì¥
#         raw = await file.read()
#         with open(in_path, "wb") as f:
#             f.write(raw)
#         log.info("ğŸ“„ ì €ì¥ ì™„ë£Œ: %s (%d bytes)", in_path, len(raw))

#         # 2) ì˜¤ë””ì˜¤ â†’ WAV(ffmpeg)
#         ffmpeg_to_wav(in_path, wav_path, sr=16000)

#         # 3) STT
#         log.info("ğŸ—£ï¸ STT ì‹œì‘")
#         segments, info = stt_model.transcribe(wav_path)
#         texts = []
#         for seg in segments:
#             t = getattr(seg, "text", None)
#             if t is None and isinstance(seg, dict):
#                 t = seg.get("text")
#             if t:
#                 texts.append(t.strip())
#         question_text = " ".join(texts) if texts else "(ìŒì„±ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
#         log.info("ğŸ—£ï¸ STT ê²°ê³¼ ê¸¸ì´=%d", len(question_text))

#         # Decision
#         decision = await llm_route_decision(question_text)
#         log.info("ğŸ§­ ë¼ìš°íŒ…: %s (%s)", decision.route, decision.reason)

#         if decision.route == "RAG":
#             log.info("ğŸ” RAG ê²€ìƒ‰ ì‹œì‘")
#             rag_resp = chat_rag(Query(question=question_text))
#             used = rag_resp.get("used_files", [])
#             answer = rag_resp.get("answer", "")
#             log.info("âœ… RAG ì™„ë£Œ, ì‚¬ìš© íŒŒì¼: %s", used)

#             hist = get_session_history(session_id)
#             log.info("ğŸ•˜ BEFORE RAG   history_len=%d", len(hist.messages))
#             hist.add_user_message(question_text)
#             hist.add_ai_message(answer)
#             log.info("ğŸ•™ AFTER  RAG   history_len=%d", len(hist.messages))

#             return {"question": question_text, "answer": rag_resp["answer"], "used_files": used}

#         if decision.route == "DIRECT":
#             log.info("ğŸ’¬ DIRECT ë‹µë³€ ìƒì„±")
#             answer = await llm_direct_answer(question_text, session_id=session_id)
#             return {"question": question_text, "answer": answer, "used_files": []}
    
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"voice-chat failed: {e}")

#     finally:
#         for p in (in_path, wav_path):
#             try:
#                 if p and os.path.exists(p):
#                     os.remove(p)
#             except:
#                 pass
